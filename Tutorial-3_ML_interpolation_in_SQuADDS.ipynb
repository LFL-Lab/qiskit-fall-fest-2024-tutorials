{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 3: Machine Learning with `SQuADDS`\n",
    "\n",
    "In this tutorial, we will walk you through how to use SQuADDS to create ML interpolation solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Training Data from `SQuADDS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will be trying to predict the design space variables of a qubit-cavity system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds import SQuADDS_DB, Analyzer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQuADDS_DB()\n",
    "db.select_system([\"qubit\",\"cavity_claw\"])\n",
    "db.select_qubit(\"TransmonCross\")\n",
    "db.select_cavity_claw(\"RouteMeander\")\n",
    "db.select_resonator_type(\"quarter\")\n",
    "merged_df = db.create_system_df()\n",
    "analyzer = Analyzer(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we need all Hamiltonian parameters to generate a **complete** training dataset. For this tutorial, I have chosen some demo targets to generate the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data_df = pd.read_csv('data/seed_data.csv')\n",
    "seed_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the training data using this `seed_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds.interpolations.utils import generate_qubit_cavity_training_data\n",
    "\n",
    "training_df = generate_qubit_cavity_training_data(analyzer, seed_data_df,\"data/training_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `training_df` has information about both the design space variables (**our targets**) and its corresponding Hamiltonian parameters (**our features**).\n",
    "\n",
    "Now, we are ready to train an ML model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the usual \"suspects\" in the ML world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from huggingface_hub import hf_hub_download\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_parquet(\"data/training_data.parquet\")\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there should not be any duplicates in the training data, we will remove them just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.drop_duplicates()\n",
    "\n",
    "# reset the index\n",
    "training_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data into features (`X` - the Hamiltonian parameters) and targets (`y` - the design space variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamiltonian_parameters = ['qubit_frequency_GHz', 'anharmonicity_MHz', 'cavity_frequency_GHz', 'kappa_kHz', 'g_MHz']\n",
    "design_parameters = ['cross_length', 'claw_length','coupling_length', 'total_length','ground_spacing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_df[hamiltonian_parameters].values # Hamiltonian parameters\n",
    "y = training_df[design_parameters].values # Design parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features help capture non-linear relationships by generating combinations of input features ($n$) raised to powers up to a specified degree ($d$). \n",
    "\n",
    "The resulting feature set includes original features, squared terms, and interaction terms (size $ \\binom{n+d}{d}$), allowing linear models to fit more complex patterns. \n",
    "\n",
    "We will use the `PolynomialFeatures` class from `sklearn.preprocessing` to generate polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Save the polynomial feature transformer\n",
    "joblib.dump(poly, 'models/poly_transformer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to normalize both the features and the target values. This ensures that all data is on the same scale, which helps the model learn more effectively. We use `StandardScaler` from `sklearn.preprocessing` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_poly = scaler_X.fit_transform(X_train_poly)\n",
    "X_test_poly = scaler_X.transform(X_test_poly)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "\n",
    "# Save the scalers\n",
    "joblib.dump(scaler_X, 'models/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, 'models/scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Training:\n",
    "\n",
    "### Simple Deep Neural Network (DNN)\n",
    "\n",
    "We'll begin with a simple deep neural network (DNN) to predict the design space variables (`y`) from the Hamiltonian parameters (`X`).\n",
    "\n",
    "- The model consists of three hidden layers:\n",
    "  - 256, 128, and 64 neurons, respectively.\n",
    "  - Each layer uses ReLU activation.\n",
    "- To improve generalization and reduce overfitting:\n",
    "  - **Batch Normalization** is applied after each layer.\n",
    "  - **Dropout (30%)** is applied after each layer.\n",
    "- The output layer matches the number of design variables.\n",
    "- The optimizer used is **Adam** with a learning rate of 0.001, and the loss function is **mean squared error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dnn_model(neurons1=256, neurons2=128, neurons3=64, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=X_train_poly.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(neurons2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(neurons3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(y_train.shape[1]))  # Output layer with the same number of neurons as output features\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the DNN model on the training data for up to 500 epochs.\n",
    "\n",
    "To ensure we save the best performing model, we have added a **ModelCheckpoint** callback that saves the model whenever the validation loss improves.\n",
    "\n",
    "Additionally, we’ve added an **EarlyStopping** callback to stop training if the validation loss doesn’t improve for 10 consecutive epochs. This helps prevent overfitting and reduces unnecessary training time by restoring the model's weights to the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for early stopping and model checkpoint\n",
    "model_checkpoint = ModelCheckpoint('models/simple_dnn.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, mode='min')\n",
    "\n",
    "# Train the model on the entire training data with callbacks\n",
    "dnn = simple_dnn_model()\n",
    "history = dnn.fit(X_train_poly, y_train, epochs=500, batch_size=16, verbose=1, validation_split=0.2, callbacks=[model_checkpoint,early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training history to see how the model performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('models/dnn_training_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('figures/dnn_training_validation_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test data\n",
    "test_mse = dnn.evaluate(X_test_poly, y_test, verbose=0)\n",
    "\n",
    "# Predictions\n",
    "y_pred = dnn.predict(X_test_poly)\n",
    "\n",
    "\n",
    "# Inverse transform the predictions and actual values to get them back to original scale\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "y_test = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs actual values for each target variable\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(y_test.shape[1]):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.scatter(y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "    plt.plot([min(y_test[:, i]), max(y_test[:, i])], [min(y_test[:, i]), max(y_test[:, i])], 'r')\n",
    "    plt.title(f'Target Variable: {design_parameters[i]}')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/dnn_predicted_vs_actual.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool to see that such a basic model can generally capture the trends in the data (sort of haha). Of course, we can always improve the model by tuning the hyperparameters, adding more data, or using more sophisticated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model performs on some different data points so that we can compare it to the scaling interpolation algorithm and the closest simulation results. First, lets load the test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f\"data/test_data.csv\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to predict the design space variables for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract input features\n",
    "X_test = test_data[hamiltonian_parameters].values\n",
    "\n",
    "# Transform input features\n",
    "X_test_poly = poly.transform(X_test)\n",
    "X_test_poly = scaler_X.transform(X_test_poly)\n",
    "\n",
    "# Make predictions with the DNN model\n",
    "y_pred_dnn = scaler_y.inverse_transform(dnn.predict(X_test_poly))\n",
    "\n",
    "# save the predictions for future use\n",
    "np.savetxt(\"data/y_pred_dnn.csv\", y_pred_dnn, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the corresponding scaling interpolation and closest simulation data points for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_df = pd.read_csv(\"data/scaling_interp_data.csv\", index_col=0)\n",
    "interp_df.columns = ['total_length', 'coupling_length', 'cross_length', 'claw_length', 'Ej', 'ground_spacing']\n",
    "\n",
    "# Sort to match the order of target_names\n",
    "scaling_interp_pred = interp_df[design_parameters].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_df = pd.read_csv(\"data/closest_sim_data.csv\", index_col=0)\n",
    "closest_df.columns = ['total_length', 'coupling_length', 'cross_length', 'claw_length', 'ground_spacing', 'Ej']\n",
    "\n",
    "# Sort to match the order of target_names\n",
    "closest_results = closest_df[design_parameters].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moments of truth! Let's see how the model performs compared to the scaling (physics) interpolation and closest simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "colors = sns.color_palette(\"husl\", 3)  # Get a palette with 3 different hues\n",
    "\n",
    "# Plot comparisons of predicted values\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "for i, target_name in enumerate(design_parameters):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(y_pred_dnn[:, i], label='ML', color=colors[0], linewidth=2)\n",
    "    plt.plot(scaling_interp_pred[:, i], label='Physics', color=colors[1], linewidth=2, linestyle='--')\n",
    "    plt.plot(closest_results[:, i], label='Closest', color=colors[2], linewidth=2, linestyle='-.')\n",
    "    plt.ylabel(r'$\\mu m$', fontsize=16)\n",
    "    # Adding title and customizing fonts\n",
    "    plt.title(f'{target_name}', fontsize=20, weight='bold')\n",
    "    plt.xlabel('Targets', fontsize=16)\n",
    "\n",
    "    # Improve legends\n",
    "    plt.legend(loc='upper left', fontsize=12, fancybox=True, framealpha=0.5)\n",
    "\n",
    "    # Adding grid and minor ticks for better readability\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/comparison_predicted_values.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate and Benchmark the ML Model\n",
    "\n",
    "To truly evaluate the model's performance, we need to simulate the qubit-cavity system using the predicted design space variables and compute the corresponding Hamiltonian parameters and see how they compare to the target Hamiltonian parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from squadds import SQuADDS_DB, Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQuADDS_DB()\n",
    "db.select_system([\"qubit\",\"cavity_claw\"])\n",
    "db.select_qubit(\"TransmonCross\")\n",
    "db.select_cavity_claw(\"RouteMeander\")\n",
    "db.select_resonator_type(\"quarter\")\n",
    "merged_df = db.create_system_df()\n",
    "analyzer = Analyzer(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dnn = np.loadtxt(\"data/y_pred_dnn.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following method to extract the `designs_df` that we will use to simulate the qubit-cavity system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds.interpolations.utils import get_design_from_ml_predictions\n",
    "\n",
    "designs_df = get_design_from_ml_predictions(analyzer, test_data, y_pred_dnn)\n",
    "designs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ansys Simulation\n",
    "\n",
    "Now to simulate each design in the `designs_df` in Ansys and extract the Hamiltonian parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squadds import AnsysSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_params_simmed = []\n",
    "\n",
    "for index, row in designs_df.iterrows():\n",
    "    print(f\"Simulating design {index}\")\n",
    "    df = designs_df.iloc[index]\n",
    "    ansys_simulator = AnsysSimulator(analyzer, df)\n",
    "    ansys_results = ansys_simulator.simulate(df)\n",
    "    hamiltonian_results = ansys_results[\"sim_results\"]\n",
    "    H_params_simmed.append(hamiltonian_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model used in this example but with its hyperparameters optimized, I was able to get the following results\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <div style=\"display: inline-flex; justify-content: center; align-items: center; gap: 20px;\">\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "      <img src=\"figures/ga.png\" alt=\"Figure 1\" style=\"width: 80%;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; text-align: center;\">\n",
    "      <img src=\"figures/kc.png\" alt=\"Figure 2\" style=\"width: 80%;\">\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `palace` Simulations\n",
    "\n",
    "If you don't have access to Ansys, you can use the [`palace`](https://awslabs.github.io/palace/) simulator to simulate the qubit-cavity system.\n",
    "\n",
    "We are actively working on developing robust, accurate, and stable simulations using the `palace` backend in collaboration with our friends at the [SQDLab](https://www.sqdlab.org/). \n",
    "\n",
    "In the meantime, we encourage you to explore `palace` on your own with the following resources:\n",
    "\n",
    "- **[Palace Documentation](https://awslabs.github.io/palace/):** Official documentation for the `palace` simulator.\n",
    "- **[Palace Installation Guide](https://lfl-lab.github.io/SQuADDS/source/resources/palace.html):** Step-by-step instructions on how to install `palace` on all platforms.\n",
    "- **[Palace Simulation with Qiskit Metal](https://github.com/sqdlab/SQDMetal):** A simulation framework for using `palace` from `qiskit-metal`.\n",
    "- **[SQDMetal Example workflow with Palace](https://github.com/sqdlab/SQDMetal/blob/main/docs/User/WorkedExamples/README.md):** A worked example of using `palace` with `qiskit-metal` to run eigenmode and capacitance simulations.\n",
    "\n",
    "These resources should help you get started with `palace` and enable you to perform simulations effectively until our enhanced integration is ready.\n",
    "\n",
    "**The next release of `SQuADDS` will include integration with `SQDMetal` to handle simulations using `palace`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMSOL Simulation\n",
    "\n",
    "You can also use `SQDMetal` to run the simulations in COMSOL as well. Read more about it [here](https://github.com/sqdlab/SQDMetal/blob/main/docs/User/Sim_Comsol.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the ML Model to HuggingFace\n",
    "\n",
    "Once you have developed a model that you are happy with and ideally performs really well, you can deploy it to HuggingFace for others to use.\n",
    "\n",
    "HuggingFace makes it ridiculously easy to deploy an ML model. All you need to do is:\n",
    "\n",
    "1. Load the model.\n",
    "2. Save the model in the format required by HuggingFace.\n",
    "\n",
    "```python\n",
    "model.save(f\"hf://{hf_username}/{model_name}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_dnn = load_model(\"models/simple_dnn.keras\")\n",
    "model_dnn.save(\"hf://shanto268/qiskit-fall-fest-2024-test-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model is **REALLYYYYY good**, then send me an [email](mailto:shanto@usc.edu) with the link to your model and I will add it to the SQuADDS ML model collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps...\n",
    "\n",
    "Please contribute to [SQuADDS!](https://lfl-lab.github.io/SQuADDS/source/resources/contribute.html)\n",
    "\n",
    "Best of luck with your final project and we look forward to seeing what you create!\n",
    "\n",
    "Apply to [USC Physics PhD program](https://dornsife.usc.edu/physics/)! ✌ ️(Deadline: Dec 15, 2024) [**No Application Fee**]\n",
    "\n",
    "[Levenson-Falk Lab (LFL)](https://dornsife.usc.edu/lfl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width: 100%; background-color:#3cb1c2;color:#324344;padding-left: 10px; padding-bottom: 10px; padding-right: 10px; padding-top: 5px'>\n",
    "    <h3>This SQuADDS tutorial was prepared for the Qiskit Fall Fest 2024</h3>\n",
    "    <p>Developed by Sadman Ahmed Shanto</p>\n",
    "    <p>This tutorial is written by Sadman Ahmed Shanto</p> \n",
    "    <p>&copy; Copyright Sadman Ahmed Shanto & Eli Levenson-Falk 2024.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
